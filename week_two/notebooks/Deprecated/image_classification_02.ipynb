{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34cc25a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554d36d6",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "We're going to try to reproduce the paper: [Backpropagation Applied to Handwritten Zip Code Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf). It won't be exact. We'll make some changes when it makes the method easier to understand.\n",
    "\n",
    "## Data\n",
    "\n",
    "We're going to cover the MNIST dataset. It's a series of $28 \\times 28$ black and white images of handwritten digits from 0 to 9.\n",
    "\n",
    "## Model\n",
    "\n",
    "The model we'll develop is from the paper above.\n",
    "It is now called the LeNet model.\n",
    "\n",
    "## Training\n",
    "\n",
    "We'll use stochastic gradient descent and we'll talk about it in more detail when we get to this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c53adf",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "MNIST stands for **m**odified **N**ational **I**nstitute of **S**tandards and **T**echnology database.\n",
    "This is a database of handwritten digits from 0 to 9.\n",
    "There are\n",
    "- 60,000 training images with corresponding labels\n",
    "- 10,000 test images with corresponding labels\n",
    "\n",
    "The images are black and white, so have 1 color channel with size $28 \\times 28$, that is 28 pixels high and 28 pixels wide.\n",
    "The original dataset stored the images in a proprietary format, instead of the `.jpg` format.\n",
    "\n",
    "Let's look at some examples of some images and labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3769e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_norm = torchvision.datasets.MNIST(root='./data',\n",
    "                                         transform=torchvision.transforms.ToTensor(),\n",
    "                                         download=True)\n",
    "mnist_norm_dl = torch.utils.data.DataLoader(mnist_norm, batch_size=len(mnist_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09837f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, _ in mnist_norm_dl:\n",
    "    mean = torch.mean(data, (0, 2, 3))\n",
    "    std = torch.std(data, (0, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25452213",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize(mean=mean, std=std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6cf8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = torchvision.datasets.MNIST(root='./data',\n",
    "                                         transform=transforms,\n",
    "                                         download=True)\n",
    "mnist_test = torchvision.datasets.MNIST(root='./data',\n",
    "                                        transform=transforms,\n",
    "                                        download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91530702",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_dl = torch.utils.data.DataLoader(mnist_train, batch_size=len(mnist_train), shuffle=True)\n",
    "mnist_test_dl = torch.utils.data.DataLoader(mnist_test, batch_size=len(mnist_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98355ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(mnist_train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88f11e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = torch.randint(low=0, high=images.shape[0], size=(1,)).item()\n",
    "plt.figure()\n",
    "plt.imshow(images[num].squeeze(0))\n",
    "plt.title(labels[num].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a2921e",
   "metadata": {},
   "source": [
    "So, we loaded the data and we showed what some of the images look like.\n",
    "Great, but how did we do it.\n",
    "That's what we're going to go in more detail in a moment.\n",
    "\n",
    "Pytorch is a great deep learning library, because it has a large amount of helper functions.\n",
    "The first ones we'll go through is\n",
    "`torch.utils.data.Dataset` and `torchvision.datasets.MNIST`.\n",
    "\n",
    "`torch.utils.data.Dataset` is an **abstract class**.\n",
    "All the other Pytorch dataset classes are subclasses of this class.\n",
    "There are two methods that are the key to this class:\n",
    "- `__getitem__`\n",
    "- `__len__`\n",
    "\n",
    "These are called **dunder** methods (dunder for double underscore).\n",
    "The `__getitem__` is the method you call when you want to get another element from the dataset.\n",
    "This method needs to be implemented when you create your own custom dataset class.\n",
    "\n",
    "The other dunder method is `__len__`.\n",
    "You call this method when you want to know how many elements are in your dataset.\n",
    "\n",
    "We'll go through an example of this.\n",
    "Recall that we called `torchvision.datasets.CIFAR10` the other day.\n",
    "If we open up the [source code](https://pytorch.org/vision/stable/_modules/torchvision/datasets/cifar.html#CIFAR10), we'll see that it's a subclass of `VisionDataset`, which is a subclass of `torch.utils.data.Dataset`.\n",
    "Our dunder methods `__getitem__` and `__len__` are not implemented.\n",
    "If you try to call them you will get a `NotImplementedError`.\n",
    "\n",
    "For `torchvision.datasets.CIFAR10`, the `__getitem__` is implemented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64816108",
   "metadata": {},
   "source": [
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e383cc42",
   "metadata": {},
   "source": [
    "The `CIFAR10` class has a class object called `data`.\n",
    "This is where the data comes from and it's also where a lot of the complexity of getting the data is from.\n",
    "The great thing about classes is that it doesn't matter if the difficulty is there!\n",
    "We can still get the item!\n",
    "\n",
    "Finally, the `__len__` is implemented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d7055",
   "metadata": {},
   "source": [
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b4ee1",
   "metadata": {},
   "source": [
    "All datasets in Pytorch use this same format.\n",
    "Pytorch also gives some useful helper classes, because the data is in a folder and the label of the data is the folder class.\n",
    "\n",
    "Now, let's look at `torchvision.datasets.MNIST`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ae34369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.datasets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=None,\n",
    "                           target_transform=None,\n",
    "                           download=True\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddec353a",
   "metadata": {},
   "source": [
    "Let's go through the different parameters of the dataset and tell you what they mean:\n",
    "- `root`: this is the location of the MNIST files or where you want these files to be downloaded.\n",
    "- `train`: as Mason talked about last week, our datasets are split into a `train`, `test`, and `val` set. If we set `train=True`, then we select the `train` dataset.\n",
    "- `transform`: this tells us how to transform the input portion of the dataset; in image classification, this would transform the image in almost any manner you want.\n",
    "- `target_transform`: this transforms the label, which in image classification is number the model is trying to predict.\n",
    "- `download`: we might already have the dataset on our computer, but if not, then we can set this to `True` and when we call the class, it will download the data along with preparing the data.\n",
    "\n",
    "So, anything that is a subclass of `torch.utils.data.Dataset` is sufficient to train a neural network! We'll train a small model to show you that this works, but later, we'll learn why this is not the most efficient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df638965",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                           train=True,\n",
    "                                           transform=torchvision.transforms.ToTensor())\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                          train=False,\n",
    "                                          transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fca3c0",
   "metadata": {},
   "source": [
    "We now want to define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac566151",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 20, 3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(20, 64, 3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Flatten(0, -1),\n",
    "    torch.nn.Linear(36864, 1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1024, 10),\n",
    "    torch.nn.LogSoftmax(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "282fa851",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83c6d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, opt, epochs=10):\n",
    "    history = []\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(\"Epoch | Batch | Time(s) | Loss\")\n",
    "    print(\"------------------------------\")\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    t = 0\n",
    "    for epoch in range(epochs):        \n",
    "        for i, (inputs, label) in enumerate(train_loader, 0):\n",
    "            inputs = inputs.to(device)\n",
    "            label = torch.tensor(label, device=device, dtype=torch.int64)\n",
    "            \n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, label)\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            step += 1\n",
    "            history.append((step, loss))\n",
    "            \n",
    "            if i % 1000 == 999:\n",
    "                t = time.time() - t - start\n",
    "                print(f\"{epoch + 1:5d} | {i+1:5d} | {int(t):8d} | {loss.item():.5f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1c9ea97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Batch | Time(s) | Loss\n",
      "------------------------------\n",
      "    1 |  1000 |       10 | 0.02356\n",
      "    1 |  2000 |        5 | 0.39154\n",
      "    1 |  3000 |       16 | 0.06587\n",
      "    1 |  4000 |       11 | 0.00002\n",
      "    1 |  5000 |       21 | 0.00206\n",
      "    1 |  6000 |       16 | 0.00556\n",
      "    1 |  7000 |       27 | 0.02085\n",
      "    1 |  8000 |       22 | 0.00364\n",
      "    1 |  9000 |       32 | 0.01517\n",
      "    1 | 10000 |       27 | 0.01362\n",
      "    1 | 11000 |       38 | 0.00003\n",
      "    1 | 12000 |       33 | 0.00005\n",
      "    1 | 13000 |       43 | 0.00153\n",
      "    1 | 14000 |       38 | 0.06796\n",
      "    1 | 15000 |       49 | 0.03803\n",
      "    1 | 16000 |       44 | 0.01876\n",
      "    1 | 17000 |       54 | 0.00063\n",
      "    1 | 18000 |       49 | 0.00011\n",
      "    1 | 19000 |       60 | -0.00000\n",
      "    1 | 20000 |       55 | 0.00003\n",
      "    1 | 21000 |       65 | 0.00308\n",
      "    1 | 22000 |       60 | 0.00001\n",
      "    1 | 23000 |       71 | 0.00008\n",
      "    1 | 24000 |       66 | 0.00106\n",
      "    1 | 25000 |       76 | 0.00004\n",
      "    1 | 26000 |       72 | 0.00013\n",
      "    1 | 27000 |       82 | 0.00054\n",
      "    1 | 28000 |       77 | 0.00000\n",
      "    1 | 29000 |       87 | 0.00013\n",
      "    1 | 30000 |       83 | 0.00903\n",
      "    1 | 31000 |       93 | 0.00036\n",
      "    1 | 32000 |       88 | 0.00240\n",
      "    1 | 33000 |       98 | 0.00008\n",
      "    1 | 34000 |       94 | 0.00397\n",
      "    1 | 35000 |      104 | 0.00007\n",
      "    1 | 36000 |      100 | 0.00026\n",
      "    1 | 37000 |      110 | 0.00021\n",
      "    1 | 38000 |      105 | 0.00033\n",
      "    1 | 39000 |      115 | 0.00046\n",
      "    1 | 40000 |      111 | 0.19587\n",
      "    1 | 41000 |      121 | 0.00014\n",
      "    1 | 42000 |      116 | 0.07540\n",
      "    1 | 43000 |      127 | 0.00142\n",
      "    1 | 44000 |      122 | 0.00071\n",
      "    1 | 45000 |      132 | -0.00000\n",
      "    1 | 46000 |      128 | 0.00003\n",
      "    1 | 47000 |      138 | 0.00000\n",
      "    1 | 48000 |      133 | 0.00133\n",
      "    1 | 49000 |      144 | 0.00010\n",
      "    1 | 50000 |      139 | 0.00200\n",
      "    1 | 51000 |      149 | 0.00000\n",
      "    1 | 52000 |      145 | 0.00082\n",
      "    1 | 53000 |      155 | 0.00012\n",
      "    1 | 54000 |      150 | 1.42193\n",
      "    1 | 55000 |      160 | 0.00058\n",
      "    1 | 56000 |      156 | 0.00000\n",
      "    1 | 57000 |      166 | 0.00252\n",
      "    1 | 58000 |      162 | -0.00000\n",
      "    1 | 59000 |      172 | -0.00000\n",
      "    1 | 60000 |      167 | 0.00097\n",
      "    2 |  1000 |      178 | 0.00043\n",
      "    2 |  2000 |      173 | 0.02645\n",
      "    2 |  3000 |      183 | 0.00003\n",
      "    2 |  4000 |      179 | 0.00006\n",
      "    2 |  5000 |      189 | 0.00000\n",
      "    2 |  6000 |      184 | 0.00407\n",
      "    2 |  7000 |      194 | 0.00385\n",
      "    2 |  8000 |      190 | 0.00019\n",
      "    2 |  9000 |      200 | 0.00038\n",
      "    2 | 10000 |      195 | 0.00000\n",
      "    2 | 11000 |      205 | -0.00000\n",
      "    2 | 12000 |      201 | -0.00000\n",
      "    2 | 13000 |      211 | 0.00010\n",
      "    2 | 14000 |      207 | 0.00323\n",
      "    2 | 15000 |      216 | 0.00050\n",
      "    2 | 16000 |      212 | 0.00041\n",
      "    2 | 17000 |      222 | 0.00182\n",
      "    2 | 18000 |      218 | 0.00006\n",
      "    2 | 19000 |      228 | -0.00000\n",
      "    2 | 20000 |      223 | -0.00000\n",
      "    2 | 21000 |      233 | 0.00001\n",
      "    2 | 22000 |      229 | 0.00000\n",
      "    2 | 23000 |      239 | -0.00000\n",
      "    2 | 24000 |      235 | 0.00006\n",
      "    2 | 25000 |      245 | 0.00008\n",
      "    2 | 26000 |      240 | 0.00000\n",
      "    2 | 27000 |      250 | 0.00044\n",
      "    2 | 28000 |      246 | 0.00000\n",
      "    2 | 29000 |      256 | 0.00000\n",
      "    2 | 30000 |      252 | 0.00836\n",
      "    2 | 31000 |      261 | 0.00000\n",
      "    2 | 32000 |      257 | 0.00005\n",
      "    2 | 33000 |      267 | 0.00000\n",
      "    2 | 34000 |      263 | 0.00129\n",
      "    2 | 35000 |      272 | 0.00001\n",
      "    2 | 36000 |      268 | 0.00270\n",
      "    2 | 37000 |      278 | 0.00000\n",
      "    2 | 38000 |      274 | 0.00016\n",
      "    2 | 39000 |      284 | 0.00002\n",
      "    2 | 40000 |      280 | 0.00094\n",
      "    2 | 41000 |      290 | 0.00000\n",
      "    2 | 42000 |      285 | 0.02995\n",
      "    2 | 43000 |      295 | 0.00001\n",
      "    2 | 44000 |      291 | 0.00009\n",
      "    2 | 45000 |      301 | -0.00000\n",
      "    2 | 46000 |      297 | 0.00002\n",
      "    2 | 47000 |      306 | -0.00000\n",
      "    2 | 48000 |      302 | 0.00196\n",
      "    2 | 49000 |      312 | 0.00000\n",
      "    2 | 50000 |      308 | 0.00000\n",
      "    2 | 51000 |      318 | 0.00000\n",
      "    2 | 52000 |      314 | 0.00001\n",
      "    2 | 53000 |      323 | -0.00000\n",
      "    2 | 54000 |      319 | 0.00019\n",
      "    2 | 55000 |      329 | 0.00165\n",
      "    2 | 56000 |      325 | -0.00000\n",
      "    2 | 57000 |      334 | 0.00001\n",
      "    2 | 58000 |      330 | -0.00000\n",
      "    2 | 59000 |      340 | -0.00000\n",
      "    2 | 60000 |      336 | 0.00000\n"
     ]
    }
   ],
   "source": [
    "history = train(model, train_dataset, criterion, opt, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c877565",
   "metadata": {},
   "source": [
    "What did we notice about that training?\n",
    "It took a lot longer than the other training!\n",
    "Part of this is because we're actually more concerned with training steps, so in this case, we did 120,000 training steps, whereas yesterday, we did 23 training steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e6a5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, label in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            label = torch.tensor(label, device=device, dtype=torch.int64)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = torch.argmax(outputs)\n",
    "            total += 1\n",
    "            correct += (predicted == label)\n",
    "            \n",
    "    print(f'Accuracy of the network on 10000 test images: {100 * correct // total} %')\n",
    "    return 100 * correct // total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "138da5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on 10000 test images: 98 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1303/3609566984.py:16: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  print(f'Accuracy of the network on 10000 test images: {100 * correct // total} %')\n",
      "/tmp/ipykernel_1303/3609566984.py:17: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return 100 * correct // total\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(98, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a129c0df",
   "metadata": {},
   "source": [
    "Now we can talk about the `DataLoader` class in Pytorch.\n",
    "This is a useful class, because it gives us a number of ways to control the computational and accelerator resources at our disposal.\n",
    "\n",
    "Let's take a look at the class and the parameters:\n",
    "\n",
    "    torch.utils.data.DataLoader(dataset, batch_size, shuffle, num_workers,\n",
    "                                pin_memory, persistent_workers)\n",
    "\n",
    "Let's go through these parameters:\n",
    "- `dataset`: this is any sublcass of the `torch.utils.data.Dataset` class. The data which you will train your model on is loaded here.\n",
    "- `batch_size`: this allows us to call several samples from the dataset at once. This is handy, because outside of model size, this will be the primary method we use to saturate the GPU.\n",
    "- `shuffle`: this determines whether we want our dataset to be shuffled after every epoch.\n",
    "- `num_workers`: this is just how many processes we use to load a batch of data to be trained.\n",
    "- `pin_memory`: this keeps your tensors in memory, which reduces the amount of time the GPU needs to call it.\n",
    "- `persistent_workers`: part of the CPU resources are used to collect elements from our dataset, transform them, and collate them. This is mainly done by the \"workers\". Setting this to `True` is a tradeoff for us. We would want to set this to `True` if the setup of our workers is a bigger drain on resources than the resources required to keep them alive.\n",
    "\n",
    "Now, let's look at what our data looks like, when it's called.\n",
    "First, we'll get data from the `torch.utils.data.Dataset` class and then we'll get data from the `torch.utils.data.DataLoader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74a01082",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f4285c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c707f",
   "metadata": {},
   "source": [
    "As you can see, we get a tensor of shape (1, 28, 28) and just a python int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0136d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                         batch_size=4,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3d73ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2c83767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1, 28, 28]), torch.Size([4]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape, label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec5bd2",
   "metadata": {},
   "source": [
    "We see that we get 4 images and 4 labels!\n",
    "\n",
    "We'll cover more datasets, such as:\n",
    "- CIFAR100\n",
    "- Fashion MNIST\n",
    "- EMNIST\n",
    "- ImageNet\n",
    "- Kuzushiji MNIST\n",
    "- Imagenette\n",
    "- Oracle MNIST\n",
    "- MS COCO\n",
    "- PASCAL VOC\n",
    "- Objects365\n",
    "\n",
    "and well cover the `torch.tensor` class in more detail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5981c20a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
