{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df77e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b68f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import shutil\n",
    "import tarfile\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0a95cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(data, class_list=None):\n",
    "    image, label = data\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "        \n",
    "    if class_list:\n",
    "        label = class_list[label]\n",
    "        \n",
    "    plt.title(label)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb601fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_history(history):\n",
    "    training_steps = np.array([step for step, _ in history])\n",
    "    loss = np.array([loss for _, loss in history])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    ax.plot(training_steps, loss)\n",
    "\n",
    "    ax.set(xlabel='training_steps', ylabel='loss')\n",
    "    ax.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be0be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(model, test_loader):\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        t = time.time()\n",
    "        for i, (inputs, label) in enumerate(test_loader):\n",
    "            inputs, label = inputs.to(device), label.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            print(label.size(0))\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "            \n",
    "            t = time.time() - t\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            print(\"Batch     | Time(s)\")\n",
    "            print(\"-------------------\")\n",
    "            print(f\"{i + 1:5d} / {len(test_loader):5d} | {int(t):7d}\")\n",
    "\n",
    "            \n",
    "    accuracy = 100 * correct / total\n",
    "            \n",
    "    print(f'Accuracy of the network on {len(test_loader.dataset)} test images: {accuracy:.1f} %')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a25aef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.1, use_cuda=True):\n",
    "    # This is code from the authors of the paper\n",
    "    # https://arxiv.org/abs/1710.09412\n",
    "    \n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        # Note, this means that MixUp does nothing.\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size()[0]\n",
    "    \n",
    "    if use_cuda:\n",
    "        # This shuffles our indices and sends these indices to the GPU\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        # This shuffles our indices without sending them to the GPU\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62225ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, dataset, epoch, opt, scheduler, loss_history=None):\n",
    "    \n",
    "    model_name = model.__class__.__name__\n",
    "    dataset_name = dataset.__class__.__name__\n",
    "    t = int(time.time())\n",
    "    \n",
    "    if dataset_name:\n",
    "        ckpt_path = '/workspace/models/' + model_name + '_' + dataset_name + f\"_{t}.ckpt\"\n",
    "    else:\n",
    "        ckpt_path = '/workspace/models/' + model_name + f\"_{t}.ckpt\"\n",
    "        \n",
    "    temp_scale_fn_custom = scheduler._scale_fn_custom\n",
    "    temp_scale_fn_ref = scheduler._scale_fn_ref\n",
    "\n",
    "    scheduler._scale_fn_custom = scheduler._scale_fn_ref()\n",
    "\n",
    "    # remove the reference so there are no more WeakMethod references in the object\n",
    "    scheduler._scale_fn_ref = None\n",
    "\n",
    "    #state = scheduler.state_dict()\n",
    "    #state.pop(\"_scale_fn_ref\")\n",
    "    \n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'loss_history': loss_history\n",
    "            }, ckpt_path)\n",
    "    \n",
    "    scheduler._scale_fn_custom = temp_scale_fn_custom\n",
    "    scheduler._scale_fn_ref = temp_scale_fn_ref\n",
    "\n",
    "    return ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4af997a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, path):\n",
    "    \n",
    "    ckpt_dict = torch.load(path)\n",
    "    \n",
    "    model.load_state_dict(ckpt_dict['model_state_dict'])\n",
    "    \n",
    "    return ckpt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd5b1cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, image, transforms, device=None, dataset=None):\n",
    "    # Image input is assumed to be an image of height, width, channel.\n",
    "    # Class list has a list of the objects classification.\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    image = transforms(image)\n",
    "    image.to(device)\n",
    "    if len(image.shape) < 4:\n",
    "        image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    \n",
    "    pred = model(image)\n",
    "    pred = torch.nn.Softmax(dim=1)(pred)\n",
    "    pred = pred.squeeze(0)\n",
    "    confidence = pred\n",
    "    pred = torch.argmax(pred).item()\n",
    "    \n",
    "    final_pred = (dataset.classes[pred], confidence[pred].item()) if dataset else (pred, confidence[pred].item())\n",
    "    \n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccacb768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_batch_train_mixup(model, images, label, use_cuda, alpha=0.1):\n",
    "    mixed_x, y_a, y_b, lam = mixup_data(images, label, alpha=alpha, use_cuda=use_cuda)\n",
    "    pred = model(mixed_x)\n",
    "\n",
    "    loss = mixup_criterion(criterion, pred, y_a, y_b, lam)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daa744e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, opt, train_dl, test_dl, start_epoch=0, scheduler=None,\n",
    "          mixup=False, mixup_pct=0.0, use_amp=False, checkpoint=0, loss_history=None,\n",
    "          use_swa=False, swa_start=0, swa_lr=3e-4, epochs=23, evaluate=True):\n",
    "    \n",
    "    # The history of our training, inputs step, loss pair.\n",
    "    \n",
    "    history = loss_history if loss_history else []\n",
    "    \n",
    "    # Initialize step\n",
    "    step = 1\n",
    "    \n",
    "    # This is the code that allows us to use our GPU to train.\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    if use_swa:\n",
    "        swa_scheduler = torch.optim.swa_utils.SWALR(opt, anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=swa_lr)\n",
    "        swa_model = torch.optim.swa_utils.AveragedModel(model)\n",
    "    \n",
    "    # This controls how many iterations we want to make, which helps control how well our model\n",
    "    # generalizes.\n",
    "    total_time = time.time()\n",
    "    \n",
    "    if use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        # Initializing the time for informational purposes.\n",
    "        t = time.time()\n",
    "        batch_start = t\n",
    "\n",
    "        #This is how we get our inputs and labels from our dataloader.\n",
    "        for i, (images, label) in enumerate(train_dl):\n",
    "            # We have to zero out our gradients, because they can get too large.\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            # This sends the inputs and the labels to our device.\n",
    "            #images, label = images.to(device), label.to(device)\n",
    "            images = images.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # This is the prediction of our model\n",
    "            if mixup and random.random() < mixup_pct:\n",
    "                loss = one_batch_train_mixup(model, images, label, use_cuda=use_cuda, alpha=0.1)\n",
    "            else:\n",
    "                pred = model(images)\n",
    "            \n",
    "                # Remember loss functions take two inputs, the prediction and the label.\n",
    "                loss = criterion(pred, label)\n",
    "            \n",
    "            # This step gets our gradients.\n",
    "            if use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                \n",
    "            if use_swa and epoch > swa_start:\n",
    "                swa_model.update_parameters(model)\n",
    "                swa_scheduler.step()\n",
    "            else:\n",
    "                # This is how we update our neural network weights. \n",
    "                opt.step()\n",
    "            \n",
    "            # Append current step and loss.\n",
    "            history.append((step, loss.item()))\n",
    "            step += 1\n",
    "\n",
    "            # Get time Batch training is taking.\n",
    "            t = time.time() - batch_start\n",
    "\n",
    "            # Gives a nice representation of how our training is progressing.\n",
    "            clear_output(wait=True)\n",
    "            print(\"Epoch | Batch | Time(s) | Loss\")\n",
    "            print(\"------------------------------\")\n",
    "            print(f\"{epoch + 1:5d} | {i + 1:5d} | {int(t):7d} | {loss:.5f}\")\n",
    "        \n",
    "        print(f\"Finished epoch {epoch + 1}.\")\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if checkpoint and ((epoch + 1) % checkpoint == 0):\n",
    "            save_model(model, train_dl.dataset, epoch, opt, scheduler, loss_history=history)\n",
    "        \n",
    "    if use_swa and epochs > swa_start:\n",
    "        torch.optim.swa_utils.update_bn(train_dl, model, device=device)\n",
    "    \n",
    "    total_time = time.time() - total_time\n",
    "    print(f\"Finished Training in {int(total_time)} seconds!\")\n",
    "    if evaluate:\n",
    "        print(\"Evaluating\")\n",
    "        model_evaluate(model, test_dl)\n",
    "        \n",
    "    print(f\"Saving model!\")\n",
    "    save_model(model, train_dl.dataset, epoch, opt, scheduler, loss_history=history)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29fad04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_onnx(model, input_shape, path=None):\n",
    "    bs, c, h, w = input_shape\n",
    "    \n",
    "    dummy_input = torch.randn(bs, c, h, w, device='cuda')\n",
    "    \n",
    "    model.to('cuda')\n",
    "    \n",
    "    input_names = [ \"input\" ]\n",
    "    output_names = [ \"output\" ]\n",
    "    \n",
    "    if not path:\n",
    "        path = model.__class__.__name__ + '.onnx'\n",
    "    \n",
    "    torch.onnx.export(model, dummy_input, path, verbose=True, input_names=input_names, output_names=output_names)\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9127a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset, transform, target_transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform=target_transform\n",
    "        self.name = 'Lakota_Plants'\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx][0]\n",
    "        label = self.dataset[idx][1]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801aa846",
   "metadata": {},
   "source": [
    "# Extract the Training and Test Data\n",
    "\n",
    "Currently we only have training data, but this will be extracted in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c2d6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if plant_data is made.\n",
    "if not os.path.isdir('/workspace/data/LAICC_2023/'):\n",
    "    os.mkdir('/workspace/data/LAICC_2023/')\n",
    "    \n",
    "if not os.path.isdir('/workspace/data/LAICC_2023/training'):\n",
    "    os.mkdir('/workspace/data/LAICC_2023/training')\n",
    "\n",
    "if not os.path.isdir('/workspace/data/LAICC_2023/test'):\n",
    "    os.mkdir('/workspace/data/LAICC_2023/test')\n",
    "\n",
    "#     for f in tqdm(iterable=tar.getmembers(), total=len(tar.getmembers())):\n",
    "#        tar.extract(member=f)\n",
    "\n",
    "# Extract data.\n",
    "if len(glob.glob('/workspace/data/LAICC_2023/training/**/*.jpg')) < 18_000: # Need better integrity check\n",
    "    with tarfile.open(\"/workspace/data/LAICC_2023/laicc_2023_training.tar.gz\", 'r:gz') as tar:\n",
    "        for f in tqdm(iterable=tar.getmembers(), total=len(tar.getmembers())):\n",
    "            tar.extract(member=f, path='/workspace/data/LAICC_2023/training')\n",
    "        # tar.extractall('/workspace/data/LAICC_2023/training')\n",
    "\n",
    "if len(glob.glob('/workspace/data/LAICC_2023/test/**/*.jpg')) < 4_000: # Need better integrity check\n",
    "    with tarfile.open(\"/workspace/data/LAICC_2023/laicc_2023_test.tar.gz\", 'r:gz') as tar:\n",
    "        for f in tqdm(iterable=tar.getmembers(), total=len(tar.getmembers())):\n",
    "            tar.extract(member=f, path='/workspace/data/LAICC_2023/test')\n",
    "        #tar.extractall('/workspace/data/LAICC_2023/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e5ad9",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "These are the variables that you will change to make your model run better or faster.\n",
    "All of your hyperparameters are below and you will make changes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de41047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'lr': 3e-3,\n",
    "    'num_classes': 7, # Don't change\n",
    "    # Decide whether you want to add mixup training and how often it is run..\n",
    "    'mixup': True,\n",
    "    'mixup_pct': 0.90,\n",
    "    # Automatic Mixed Precision, should speed up training.\n",
    "    'use_amp': True,\n",
    "    # Epochs - how long you want to train\n",
    "    'epochs': 20,\n",
    "    'start_epoch': 0,\n",
    "    # Pin the memory, this should speed up training, but could make the kernel more stable\n",
    "    'pin_memory': True, # Don't change\n",
    "    # This is your batch size.\n",
    "    'bs': 256,\n",
    "    # Whether or not to use stochastic weight averaging. Setting to true should increase test accuracy.\n",
    "    # If use_swa is set to False, then swa_start and swa_lr is not used.\n",
    "    'use_swa': True,\n",
    "    'swa_start': 17,\n",
    "    'swa_lr': 5e-3,\n",
    "    # How many epochs to train before saving model. If set to 0, checkpointing will not be performed.\n",
    "    'checkpoint': 10,\n",
    "    # Where your training data is stored.\n",
    "    'train_root': '/workspace/data/LAICC_2023/training/', # Do not change\n",
    "    # Where your test data is stored.\n",
    "    'test_root': '/workspace/data/LAICC_2023/test/', # Do not change\n",
    "    # This should be a number and how often you should save your data.\n",
    "    'num_workers': 4,\n",
    "    # If you want to restart training, change this to the path of the checkpoint you wish to start at.\n",
    "    'ckpt_path': None, #'/workspace/models/ResNet_Lakota_Plants_1658958130.ckpt',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7679cf6",
   "metadata": {},
   "source": [
    "### Transforms\n",
    "\n",
    "Define your transforms below. The minimal transform input is included below, but you can and should add more.\n",
    "Select new transforms from [Pytorch Transforms](https://pytorch.org/vision/main/transforms.html#transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fedaedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose(\n",
    "    [#T.Resize(size=(246, 246)),\n",
    "     T.CenterCrop([224]),\n",
    "     T.AugMix(),\n",
    "     #T.RandomPerspective(),\n",
    "     T.RandomHorizontalFlip(p=0.5),\n",
    "     T.RandomVerticalFlip(p=0.5),\n",
    "     T.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1)),\n",
    "     T.RandomRotation(degrees=(0, 10)),\n",
    "     #T.Grayscale(num_output_channels=3),\n",
    "     T.ToTensor(),\n",
    "     T.Normalize(mean=torch.tensor([0.485, 0.456, 0.406]), std=torch.tensor([0.229, 0.224, 0.225])),\n",
    "    ])\n",
    "\n",
    "test_transforms = T.Compose(\n",
    "    [#T.Resize(size=(246, 246)),\n",
    "     T.CenterCrop([224]),\n",
    "     #T.Grayscale(num_output_channels=3),\n",
    "     T.ToTensor(),\n",
    "     T.Normalize(mean=torch.tensor([0.485, 0.456, 0.406]), std=torch.tensor([0.229, 0.224, 0.225])),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33800b8b",
   "metadata": {},
   "source": [
    "# Initialize the Dataset\n",
    "\n",
    "We will initialize our dataset using the transforms above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71fa3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your training dataset by setting train=True\n",
    "train_ds = torchvision.datasets.ImageFolder(root=hparams['train_root'], transform=transforms,\n",
    "                                            target_transform=None, loader=Image.open,\n",
    "                                            is_valid_file=None)\n",
    "\n",
    "test_ds = torchvision.datasets.ImageFolder(root=hparams['test_root'], transform=test_transforms,\n",
    "                                            target_transform=None, loader=Image.open,\n",
    "                                            is_valid_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6eadf1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.__class__.__name__ = 'Lakota_Plants'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a409dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your dataloader\n",
    "train_dl = torch.utils.data.DataLoader(train_ds,\n",
    "                                       batch_size=hparams['bs'],\n",
    "                                       shuffle=True,\n",
    "                                       sampler=None,\n",
    "                                       num_workers=hparams['num_workers'],\n",
    "                                       persistent_workers=True,\n",
    "                                       pin_memory=hparams['pin_memory'])\n",
    "test_dl = torch.utils.data.DataLoader(test_ds,\n",
    "                                      batch_size=hparams['bs'],\n",
    "                                      shuffle=False,\n",
    "                                      sampler=None,\n",
    "                                      num_workers=hparams['num_workers'],\n",
    "                                      persistent_workers=False,\n",
    "                                      pin_memory=hparams['pin_memory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96e69aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your model from the following link:\n",
    "# https://pytorch.org/vision/stable/models.html#classification\n",
    "\n",
    "#torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "#model = torchvision.models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "model = torchvision.models.mobilenet_v3_small(weights=torchvision.models.MobileNet_V3_Small_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "739cdd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca3e8238",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, len(train_ds.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a684ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b04db266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are doing classification, we will always use this loss.\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30249e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=hparams['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(opt, base_lr=hparams['lr'], max_lr=10*hparams['lr'],\n",
    "                                          step_size_up=1000, mode='triangular',\n",
    "                                          cycle_momentum=False)\n",
    "#scheduler2 = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.95)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.ChainedScheduler([scheduler1, scheduler2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c3636",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "You can select one of the following optimizers.\n",
    "For all of these, `params=model.parameters()`\n",
    "\n",
    "- `torch.optim.Adadelta(params, lr=1, rho=0.9, eps=1e-06, weight_decay=0)`\n",
    "- `torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, eps=1e-10)`\n",
    "- `torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)` use lr=3e-4 or close to it for Adam.\n",
    "- `torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)`\n",
    "- `torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)`\n",
    "- `torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0)`\n",
    "- `torch.optim.SGD(params, lr=0.001, momentum=0, dampening=0, weight_decay=0, nesterov=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7d04b45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if hparams['ckpt_path']:\n",
    "    ckpt_dict = torch.load(hparams['ckpt_path'])\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.load_state_dict(ckpt_dict['model_state_dict'], strict=False)\n",
    "    opt.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(ckpt_dict['scheduler'])\n",
    "    hparams['start_epoch'] = ckpt_dict['epoch']\n",
    "    for state in opt.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.to(device)\n",
    "\n",
    "    \n",
    "else:\n",
    "    ckpt_dict = {\n",
    "            'epoch': 0,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'loss_history': None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ce95b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Batch | Time(s) | Loss\n",
      "------------------------------\n",
      "   10 |    28 |     153 | 1.25862\n"
     ]
    }
   ],
   "source": [
    "# Call to begin training\n",
    "# Change the value of epochs\n",
    "history = train(model, criterion, opt, train_dl, test_dl, start_epoch=hparams['start_epoch'], \n",
    "                scheduler=scheduler, mixup=hparams['mixup'], mixup_pct=hparams['mixup_pct'],\n",
    "                use_amp=hparams['use_amp'], checkpoint=hparams['checkpoint'],\n",
    "                loss_history=ckpt_dict['loss_history'], use_swa=hparams['use_swa'],\n",
    "                swa_start=hparams['swa_start'], swa_lr=hparams['swa_lr'],\n",
    "                epochs=hparams['epochs'], evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluate(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead29dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test = []\n",
    "\n",
    "for _, labels in tqdm(test_dl):\n",
    "    labels = torch.flatten(labels)\n",
    "    labels = [l.item() for l in labels]\n",
    "    y_true_test.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143fd362",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test = [item for sublist in y_true_test for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6fabd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "model.to('cuda')\n",
    "\n",
    "for image, _ in tqdm(test_dl):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    image = image.to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds = torch.nn.Softmax(dim=1)(model(image))\n",
    "        labels = torch.argmax(preds, dim=1)\n",
    "        labels = torch.flatten(labels)\n",
    "        labels = [l.item() for l in labels]\n",
    "        y_pred.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d3f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [item for sublist in y_pred for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeb6d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y_true_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6e25c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cfm / np.sum(cfm, axis=-1),\n",
    "                              display_labels=train_ds.classes)\n",
    "                              #display_labels=[i for i in range(len(base_ds.classes))])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13,13))\n",
    "disp.plot(ax=ax)\n",
    "plt.savefig('cfm_resnext_lakota_plant_data.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5cbbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make a prediction, paste the path to the image below.\n",
    "# # img_path = '/workspace/data/plant_data/Purple_Coneflower/20220716_230129.jpg'\n",
    "\n",
    "# img = Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2fc373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_prediction(model.to('cuda'), img, test_transforms, device='cuda', dataset=base_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model.evaluate(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF you are satisfied with your models performance, save to ONNX\n",
    "# You need to change your input shape. It should be of the form\n",
    "# (batch_size, num_channels, height, width)\n",
    "# You will get height and width from the transform\n",
    "# T.Resize(size=(224, 224)), which would mean we would have\n",
    "input_shape = (10, 3, 224, 224)\n",
    "\n",
    "\n",
    "path = f'/workspace/models/{model.__class__.__name__}_{cpu}_{acc:.2f}_{int(time.time())}.onnx'\n",
    "\n",
    "save_to_onnx(model, input_shape, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb009d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_path = f'/workspace/models/{model.__class__.__name__}_{cpu}_{acc:.2f}_{int(time.time())}.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, torch_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
